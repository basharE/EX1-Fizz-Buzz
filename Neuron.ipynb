{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neuron.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyONAgbpn1WMjc9tKdoRvP9s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basharE/EX1-Fizz-Buzz/blob/main/Neuron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm9wxoGHcYUu"
      },
      "source": [
        "# Question 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ7YeDVNcuWp"
      },
      "source": [
        "## Neuron Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRyqkba1Gvq_"
      },
      "source": [
        "import numpy as np\n",
        "class Neuron(): \n",
        "  \n",
        "  def __init__(self,m,e):\n",
        "\n",
        "    # seeding for random number generation   \n",
        "    np.random.seed(1)\n",
        "          \n",
        "    self.weight_matrix = np.random.normal(0,e,m)\n",
        "    \n",
        "    self.b = np.random.normal(0,e,1)[0]\n",
        "\n",
        "  # relu as activation function \n",
        "  def relu(self, x): \n",
        "    return np.maximum(0,x)\n",
        "    \n",
        "  # derivative of relu function. \n",
        "  # Needed to calculate the gradients. \n",
        "  def relu_derivative(self, x): \n",
        "    if x < 0 : return 0\n",
        "    else : return 1  \n",
        "\n",
        "  # forward propagation \n",
        "  def forward(self, inputs): \n",
        "    self.res = self.relu(np.dot(inputs, self.weight_matrix) + self.b)\n",
        "    self.x = inputs\n",
        "    return self.res\n",
        "\n",
        "  #def backward()\n",
        "  def backward(self, error): \n",
        "    dx = np.dot(error, np.transpose(self.weight_matrix))\n",
        "    db = self.b * error\n",
        "    dw = np.dot(error, np.transpose(x))\n",
        "    return db,dw,dx\n",
        "\n",
        "\n",
        "  def SetWb(self, weights, bias):\n",
        "    self.b = bias\n",
        "    self.weight_matrix = weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUe88Y_xeGsK"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL3h09kAdzkk",
        "outputId": "7678475f-1eca-4715-b82e-e240264043c5"
      },
      "source": [
        "# init assumption  is that the activation  function is Relu\n",
        "n = Neuron(10,0.1)\n",
        "\n",
        "x = np.random.normal(0,0.1,10)\n",
        "\n",
        "# given x compute the result and maintain result in the object\n",
        "res = n.forward(x)\n",
        "\n",
        "# given the error compute the derivatives\n",
        "db,dw,dx = n.backward(0.3)\n",
        "print(res,db,dw,dx)\n",
        "\n",
        "x = np.ones(10)\n",
        "\n",
        "w = np.array([1,2,3,4,5,6,7,8,9,10])\n",
        "\n",
        "# change w and b\n",
        "n.SetWb(w,3)\n",
        "\n",
        "res = n.forward(x)\n",
        "\n",
        "db,dw,dx = n.backward(0.3)\n",
        "\n",
        "print(res,db,dw,dx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0879985524215702\n",
            "0.0879985524215702 0.04386323811134922 [-0.06180422 -0.00967252 -0.01152163  0.03401308 -0.03299674 -0.00517285\n",
            " -0.02633575  0.00126641  0.01748446 -0.03301858] [ 0.04873036 -0.01835269 -0.01584515 -0.03218906  0.02596223 -0.06904616\n",
            "  0.05234435 -0.02283621  0.00957117 -0.00748111]\n",
            "58.0 0.8999999999999999 [0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3] [0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 3. ]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}